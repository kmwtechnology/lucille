# This is a configuration file for indexing project files into OpenSearch with vector capabilities
# It is in HOCON format, a superset of JSON

connectors: [
  {
    name: "fileConnector",
    class: "com.kmwllc.lucille.connector.FileConnector",
    pipeline: "data_load",
    # Path to the Lucille project directory (will be set as environment variable)
    pathsToStorage: [${PROJECT_PATH}]

    # Simple text extraction should be false since we're using Tika for extraction
    fileOptions {
      getFileContent: false
    }
    filterOptions {
      includes: [
        ".*\\.txt$", ".*\\.pdf$", ".*\\.docx$", ".*\\.doc$", ".*\\.rtf$", ".*\\.odt$",
        ".*\\.html$", ".*\\.htm$", ".*\\.xml$", ".*\\.json$", ".*\\.md$",
        ".*\\.ppt$", ".*\\.pptx$", ".*\\.xls$", ".*\\.xlsx$", ".*\\.epub$",
        ".*\\.java$", ".*\\.py$", ".*\\.csv$", ".*\\.pptm$", ".*\\.xlsm$", ".*\\.docm$",
        ".*\\.ods$", ".*\\.odp$", ".*\\.odg$", ".*\\.odf$", ".*\\.ipynb$", ".*\\.adoc$"
      ]
      excludes: [
        ".*\\.log$", ".*\\.tmp$", ".*\\.DS_Store$"
      ]
    }
  }
]

pipelines: [
  {
    name: "data_load",
    stages: [
      # Extract Text with Tika - using correct file_path field from FileConnector
      {
        name: "extractContent"
        class: "com.kmwllc.lucille.tika.stage.TextExtractor"
        file_path_field: "file_path"
        text_field: "text"
        metadata_prefix: "tika"
      },
      # Extract filename from Tika metadata if available
      {
        name: "extractFilename",
        class: "com.kmwllc.lucille.stage.CopyFields"
        source: ["tika_resourceName"]
        dest: ["filename"]
        update_mode: "skip"  # Only use Tika value if filename isn't already set
      },
      # Parse file path to extract file extension
      {
        name: "parseFilePath",
        class: "com.kmwllc.lucille.stage.ParseFilePath"
        filePathField: "file_path"
        uppercaseExtension: false
      },
      # Rename extracted fields
      {
        name: "renameFields",
        class: "com.kmwllc.lucille.stage.RenameFields"
        fieldMapping: {
          "text": "content",
          "path": "full_path",
          "name": "filename"
        }
        update_mode: "overwrite"
      },
      # Add default content type if none exists
      {
        name: "addDefaultContentType",
        class: "com.kmwllc.lucille.stage.SetStaticValues"
        static_values: {
          "content_type": "application/octet-stream"
        }
        update_mode: "skip"
      },
      # Copy Tika content type field
      {
        name: "copyContentType",
        class: "com.kmwllc.lucille.stage.CopyFields"
        source: ["tika_content_type"]
        dest: ["content_type"]
        update_mode: "overwrite"  # Changed from "skip" to "overwrite" to ensure Tika values take precedence
      },
      # Copy the content to chunk_text field for embedding
      {
        name: "copyToChunk",
        class: "com.kmwllc.lucille.stage.CopyFields"
        source: ["content"]
        dest: ["chunk_text"]
      },
      # Add text chunking stage to break content into smaller chunks
      {
        name: "textChunker",
        class: "com.kmwllc.lucille.stage.ChunkText"
        source: "chunk_text"
        dest: "chunk_text"
        chunking_method: "fixed"
        length_to_split: 2000
        clean_chunks: true
        chunks_to_merge: 1
        overlap_percentage: 10
      },
      # Emit chunk children as separate documents
      {
        name: "emitChunks",
        class: "com.kmwllc.lucille.stage.EmitNestedChildren"
        drop_parent: true
        fields_to_copy: {
          "path": "path"
          "file_path": "file_path"
          "full_path": "full_path"
          "filename": "filename"
          "content_type": "content_type"
          "file_extension": "file_extension"
          "file_size_bytes": "file_size_bytes"
          "file_modification_date": "file_modification_date"
          "file_creation_date": "file_creation_date"
          # "chunk_id": "chunk_id"
        }
      },
      # Generate embeddings using Google Gemini API
      # {
      #   name: "ollamaEmbed",
      #   class: "com.kmwllc.lucille.stage.PromptOllama"
      #   hostURL: ${?OLLAMA_HOST}  # e.g., "http://localhost:11434"
      #   modelName: "nomic-embed-text:latest"
      #   systemPrompt: "Generate embeddings for the following text:"
      #   fields: ["chunk_text"]
      #   requireJSON: true
      #   timeout: 30  # seconds
      #   update_mode: "overwrite"
      # }
    ],
    "destination": {
      "type": "opensearch",
      "url": ${?OPENSEARCH_URL},
      "index": ${?OPENSEARCH_INDEX}
    }
  }
]

indexer {
  type: "OpenSearch"
  batchTimeout: 2000
  batchSize: 100
  sendEnabled: true
}

opensearch {
  # OpenSearch URL will be set as environment variable
  url: ${OPENSEARCH_URL}
  # Index name will be set as environment variable
  index: ${OPENSEARCH_INDEX}
  acceptInvalidCert: true
}

worker {
  pipeline: "pipeline1"
  # Reduced to 2 threads to respect Gemini API rate limits
  threads: 16
  # # Add sleep between documents to control rate
  # sleepBetweenDocs: 40
  # # Add delay on processor errors (but do not use maxRetries to avoid ZooKeeper dependency)
  # sleepOnProcessorError: 500
}

publisher {
  queueCapacity: 100
  # Add batch size configuration for the publisher
  batchSize: 10
  # Add timeout configuration for the publisher
  batchTimeout: 10000
}

log {
  seconds: 30
}
