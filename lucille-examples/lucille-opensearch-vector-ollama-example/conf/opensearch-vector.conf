# This is a configuration file for indexing project files into OpenSearch with vector capabilities
# It is in HOCON format, a superset of JSON

connectors: [
  {
    name: "fileConnector",
    class: "com.kmwllc.lucille.connector.FileConnector",
    pipeline: "data_load",
    # Path to the Lucille project directory (will be set as environment variable)
    pathsToStorage: ["."]
    pathsToStorage: [${?PROJECT_PATH}]

    # Simple text extraction should be false since we're using Tika for extraction
    fileOptions {
      getFileContent: false
    }
    filterOptions {
      includes: [
        ".*\\.txt$", ".*\\.pdf$", ".*\\.docx$", ".*\\.doc$", ".*\\.rtf$", ".*\\.odt$",
        ".*\\.html$", ".*\\.htm$", ".*\\.xml$", ".*\\.json$", ".*\\.md$",
        ".*\\.ppt$", ".*\\.pptx$", ".*\\.xls$", ".*\\.xlsx$", ".*\\.epub$",
        ".*\\.java$", ".*\\.py$", ".*\\.csv$", ".*\\.pptm$", ".*\\.xlsm$", ".*\\.docm$",
        ".*\\.ods$", ".*\\.odp$", ".*\\.odg$", ".*\\.odf$", ".*\\.ipynb$", ".*\\.adoc$"
      ]
      excludes: [
        ".*\\.log$", ".*\\.tmp$", ".*\\.DS_Store$"
      ]
    }
  }
]

pipelines: [
  {
    name: "data_load",
    stages: [
      # Extract Text with Tika - using correct file_path field from FileConnector
      {
        name: "extractContent"
        class: "com.kmwllc.lucille.tika.stage.TextExtractor"
        file_path_field: "file_path"
        text_field: "text"
        metadata_prefix: "tika"
      },
      # Extract filename from Tika metadata if available
      {
        name: "extractFilename",
        class: "com.kmwllc.lucille.stage.CopyFields"
        source: ["tika_resourceName"]
        dest: ["filename"]
        update_mode: "skip"  # Only use Tika value if filename isn't already set
      },
      # Parse file path to extract file extension
      {
        name: "parseFilePath",
        class: "com.kmwllc.lucille.stage.ParseFilePath"
        filePathField: "file_path"
        uppercaseExtension: false
      },
      # Rename extracted fields
      {
        name: "renameFields",
        class: "com.kmwllc.lucille.stage.RenameFields"
        fieldMapping: {
          "text": "content",
          "path": "full_path",
          "name": "filename"
        }
        update_mode: "overwrite"
      },
      # Add default content type if none exists
      {
        name: "addDefaultContentType",
        class: "com.kmwllc.lucille.stage.SetStaticValues"
        static_values: {
          "content_type": "application/octet-stream"
        }
        update_mode: "skip"
      },
      # Copy Tika content type field
      {
        name: "copyContentType",
        class: "com.kmwllc.lucille.stage.CopyFields"
        source: ["tika_content_type"]
        dest: ["content_type"]
        update_mode: "overwrite"  # Changed from "skip" to "overwrite" to ensure Tika values take precedence
      },
      # Extract named entities using OpenNLP
      {
        name: "extractEntities",
        class: "com.kmwllc.lucille.stage.ApplyOpenNLPNameFinders"
        textField: "content"
        tokenizerPath: "classpath:en-token.bin"
        models: {
          person: "classpath:en-ner-person.bin"
          organization: "classpath:en-ner-organization.bin"
          location: "classpath:en-ner-location.bin"
        }
      },
      # Copy the content to chunk_text field for embedding
      {
        name: "copyToChunk",
        class: "com.kmwllc.lucille.stage.CopyFields"
        source: ["content"]
        dest: ["chunk_text"]
      },
      # Add text chunking stage to break content into smaller chunks BEFORE entity enrichment
      {
        name: "textChunker",
        class: "com.kmwllc.lucille.stage.ChunkText"
        source: "chunk_text"
        dest: "chunk_text"
        chunking_method: "fixed"
        length_to_split: 1500       # Reduced from 2000
        clean_chunks: true
        chunks_to_merge: 2          # Keep at 2
        overlap_percentage: 25      # Increased from 20%
      },
      # Emit chunk children as separate documents
      {
        name: "emitChunks",
        class: "com.kmwllc.lucille.stage.EmitNestedChildren"
        drop_parent: true
        fields_to_copy: {
          "path": "path"
          "file_path": "file_path"
          "full_path": "full_path"
          "filename": "filename"
          "content_type": "content_type"
          "file_extension": "file_extension"
          "file_size_bytes": "file_size_bytes"
          "file_modification_date": "file_modification_date"
          "file_creation_date": "file_creation_date"
          "person": "person"
          "organization": "organization"
          "location": "location"
        }
      },
      # Enrich each chunk with entity information directly into chunk_text
      {
        name: "enrichChunkTextWithEntities",
        class: "com.kmwllc.lucille.stage.Concatenate"
        dest: "chunk_text"
        format_string: "{chunk_text} | Entities: People: {person} Organizations: {organization} Locations: {location}"
        default_inputs {
          "person": ""
          "organization": ""
          "location": ""
        }
      },
      # Generate embeddings using Ollama API
      {
        name: "ollamaEmbed",
        class: "com.kmwllc.lucille.stage.EmbeddingsOllama"
        hostURL: "http://localhost:11434"
        hostURL: ${?OLLAMA_HOST}  # e.g., "http://localhost:11434"
        modelName: "nomic-embed-text:latest"
        chunk_text: "chunk_text"
        update_mode: "overwrite"
      }
    ],
    "destination": {
      "type": "opensearch",
      "url": ${?OPENSEARCH_URL},
      "index": ${?OPENSEARCH_INDEX}
    }
  }
]

indexer {
  type: "OpenSearch"
  batchTimeout: 2000
  batchSize: 100
  sendEnabled: true
}

opensearch {
  # OpenSearch URL will be set as environment variable or use default
  url: "http://localhost:9200"
  url: ${?OPENSEARCH_URL}
  # Index name will be set as environment variable or use default
  index: "test_index"
  index: ${?OPENSEARCH_INDEX}
  acceptInvalidCert: true
}

worker {
  pipeline: "pipeline1"
  threads: 4
}

publisher {
  queueCapacity: 100
}

log {
  seconds: 30
}
