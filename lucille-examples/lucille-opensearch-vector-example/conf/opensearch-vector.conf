# This is a configuration file for indexing project files into OpenSearch with vector capabilities
# It is in HOCON format, a superset of JSON

connectors: [
  {
    name: "fileConnector",
    class: "com.kmwllc.lucille.connector.FileConnector",
    pipeline: "pipeline1",
    # Path to the Lucille project directory (will be set as environment variable)
    pathToStorage: ${PROJECT_PATH}
    # Include various file types we want to process
    # includeSuffixes: ['.txt', '.pdf', '.docx', '.doc', '.rtf', '.odt',
    #         '.html', '.htm', '.xml', '.json', '.md',
    #         '.ppt', '.pptx', '.xls', '.xlsx', '.epub',
    #         '.java', '.py', '.csv', '.pptm', '.xlsm', '.docm',
    #         '.ods', '.odp', '.odg', '.odf', '.ipynb', '.adoc']
    # Exclude directories we don't want to process
    # excludePatterns: [
    #   "**/*.log"
    # ]
    # Limit the number of files to process (increase for production use)
    maxDocs: 100
    # Simple text extraction should be false since we're using Tika for extraction
    extractText: false
    # Add throttling to control processing rate and respect API limits
    throttleDelayMs: 100
    batchSize: 50 # Changed from 5 to 50 to process more files in parallel
  }
]

pipelines: [
  {
    name: "pipeline1",
    stages: [
      # Extract Text with Tika - using correct file_path field from FileConnector
      {
        name: "extractContent"
        class: "com.kmwllc.lucille.tika.stage.TextExtractor"
        file_path_field: "file_path"
        text_field: "text"
        metadata_prefix: "tika"
      },
      # Extract filename from Tika metadata if available
      {
        name: "extractFilename",
        class: "com.kmwllc.lucille.stage.CopyFields"
        source: ["tika_resourceName"]
        dest: ["filename"]
        update_mode: "skip"  # Only use Tika value if filename isn't already set
      },
      # Parse file path to extract file extension
      {
        name: "parseFilePath",
        class: "com.kmwllc.lucille.stage.ParseFilePath"
        filePathField: "file_path"
        uppercaseExtension: false
      },
      # Rename extracted fields
      {
        name: "renameFields",
        class: "com.kmwllc.lucille.stage.RenameFields"
        fieldMapping: {
          "text": "content",
          "path": "full_path",
          "name": "filename"
        }
        update_mode: "overwrite"
      },
      # Add default content type if none exists
      {
        name: "addDefaultContentType",
        class: "com.kmwllc.lucille.stage.SetStaticValues"
        static_values: {
          "content_type": "application/octet-stream"
        }
        update_mode: "skip"
      },
      # Copy Tika content type field
      {
        name: "copyContentType",
        class: "com.kmwllc.lucille.stage.CopyFields"
        source: ["tika_content_type"]
        dest: ["content_type"]
        update_mode: "overwrite"  # Changed from "skip" to "overwrite" to ensure Tika values take precedence
      },
      # Copy the content to chunk_text field for embedding
      {
        name: "copyToChunk",
        class: "com.kmwllc.lucille.stage.CopyFields"
        source: ["content"]
        dest: ["chunk_text"]
      },
      # Add text chunking stage to break content into smaller chunks
      {
        name: "textChunker",
        class: "com.kmwllc.lucille.stage.ChunkText"
        source: "chunk_text"
        dest: "chunk_text"
        chunking_method: "fixed"
        length_to_split: 2000
        clean_chunks: true
        chunks_to_merge: 1
        overlap_percentage: 10
      },
      # Add random string for chunk_id
      {
        name: "addChunkId",
        class: "com.kmwllc.lucille.stage.AddRandomString"
        field_name: "chunk_id"
        range_size: 1000000
        min_num_of_terms: 1
        max_num_of_terms: 1
      },
      # Emit chunk children as separate documents
      {
        name: "emitChunks",
        class: "com.kmwllc.lucille.stage.EmitNestedChildren"
        drop_parent: true
        fields_to_copy: {
          "path": "path"
          "file_path": "file_path"
          "full_path": "full_path"
          "filename": "filename"
          "content_type": "content_type"
          "file_extension": "file_extension"
          "file_size_bytes": "file_size_bytes"
          "file_modification_date": "file_modification_date"
          "file_creation_date": "file_creation_date"
          "chunk_id": "chunk_id"
        }
      },
      # Generate embeddings using Google Gemini API
      {
        name: "geminiEmbed",
        class: "com.kmwllc.lucille.example.GeminiEmbed"
        source: "chunk_text"
        dest: "embedding"  # Changed from chunk_vector to embedding to match Next.js expectations
        embed_document: true
        embed_children: false
        api_key: ${?GEMINI_API_KEY}
        model_name: "text-embedding-004"
        dimensions: 768
        request_timeout_millis: 30000
      }
    ],
    "destination": {
      "type": "opensearch",
      "url": ${?OPENSEARCH_URL},
      "index": ${?OPENSEARCH_INDEX}
    }
  }
]

indexer {
  type: "OpenSearch"
  batchTimeout: 2000
  batchSize: 100
  sendEnabled: true
}

opensearch {
  # OpenSearch URL will be set as environment variable
  url: ${OPENSEARCH_URL}
  # Index name will be set as environment variable
  index: ${OPENSEARCH_INDEX}
  acceptInvalidCert: true
}

worker {
  pipeline: "pipeline1"
  # Reduced to 2 threads to respect Gemini API rate limits
  threads: 2
  # Add sleep between documents to control rate
  sleepBetweenDocs: 40
  # Add delay on processor errors (but do not use maxRetries to avoid ZooKeeper dependency)
  sleepOnProcessorError: 500
}

publisher {
  queueCapacity: 100
  # Add batch size configuration for the publisher
  batchSize: 10
  # Add timeout configuration for the publisher
  batchTimeout: 10000
}

log {
  seconds: 30
}

// Pipeline for processing files with vector embeddings
pipeline {
  name: "pipeline1"
  source {
    type: "file"
    name: "fileConnector"
    path: ".."
    includes: [
      "**/*.java",
      "**/*.md",
      "**/*.yaml",
      "**/*.yml",
      "**/*.json",
      "**/*.conf",
      "**/*.properties"
    ]
    excludes: [
      "**/target/**",
      "**/test/**",
      "**/.git/**",
      "**/node_modules/**",
      "**/.idea/**",
      "**/.vscode/**",
      "**/.github/**",
      "**/.svn/**",
      "**/.hg/**",
      "**/.tox/**",
      "**/.pytest_cache/**",
      "**/.DS_Store/**",
      "dist/",
      "*.py[cod]",
      "*.iml",
      "*.project",
      ".settings",
      ".classpath",
      "TEST*.xml",
      "Thumbs.db"
    ]
    idField: "file_path"
    recursive: true
    throttleDelayMs: 100
    batchSize: 5
  }

  processors: [
    {
      name: "textExtractor"
      type: "TikaTextExtractor"
      textField: "text"
      extractAllMetadata: true
    },
    {
      name: "parseFilePath"
      type: "ParseFilePath"
      filePathField: "file_path"
      uppercaseExtension: false
    },
    {
      name: "renameFields"
      type: "RenameFields"
      renames {
        "Content-Type": "content_type"
        "resourceName": "filename"
      }
    },
    {
      name: "filePathField"
      type: "CopyField"
      source: "id"
      dest: "file_path"
    },
    {
      name: "addChunkId"
      type: "AddRandomString"
      field_name: "chunk_id"
      range_size: 1000000
      min_num_of_terms: 1
      max_num_of_terms: 1
    },
    {
      name: "copyForChunking"
      type: "CopyField"
      source: "text"
      dest: "text_to_chunk"
    },
    {
      name: "chunkText"
      type: "ChunkText"
      field: "text_to_chunk"
      outputField: "chunks"
      chunkSize: 1000
      overlapSize: 100
      idField: "file_path"
      includeParent: false
    },
    {
      name: "emitChunks"
      type: "EmitNestedChildren"
      path: "chunks"
      childrenField: "chunks"
      prefix: "chunk_"
      breakpoint: true
    },
    {
      name: "cleanup"
      type: "DropField"
      fields: ["text_to_chunk"]
    },
    {
      name: "geminiEmbed"
      type: "GeminiEmbed"
      field: "chunk_text"
      vectorField: "embedding"  # Changed from chunk_vector to embedding
      modelName: "embedding-001"
      apiKey: ${?GEMINI_API_KEY}
      dimension: 768
    }
  ]

  publisher {
    type: "opensearch"
    name: "opensearchPublisher"
    url: ${OPENSEARCH_URL}
    username: "admin"
    password: "StrongPassword123!"
    verifySsl: false
    index: ${OPENSEARCH_INDEX}
    queueCapacity: 100
    batchSize: 10
    batchTimeout: 10000
  }
}

// Worker configuration to optimize rate limiting
worker {
  pipeline: "pipeline1"
  // Reduced threads to respect Gemini API rate limits
  threads: 2
  // Add sleep between documents to control rate
  sleepBetweenDocs: 50
  // Add delay on processor errors
  sleepOnProcessorError: 500
}
