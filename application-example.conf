# This is a configuration file managed by https://github.com/lightbend/config
# It is in HOCON format, a superset of JSON

connectors: [
    {class: "com.kmwllc.lucille.connector.CSVConnector",
     path: "/Volumes/Work/lucille/src/test/resources/test.csv", name: "connector1", pipeline: "pipeline1"},
    {class: "com.kmwllc.lucille.connector.CSVConnector",
     path: "/Volumes/Work/lucille/src/test/resources/test4.csv", name: "connector2", pipeline: "pipeline2"}
]

pipelines: [{name: "pipeline1", stages: [{class: "com.kmwllc.lucille.stage.CreateChildrenStage"}]},
            {name: "pipeline2", stages: [{class: "com.kmwllc.lucille.stage.CreateChildrenStage"}]}
]

indexer {
    batchTimeout: 6000
    batchSize: 100
}

#alternative syntax for declaring pipelines:
#pipelines: [{name: "pipeline1", stages: [{class: "com.kmwllc.lucille.stage.CreateChildrenStage"}]}]
#pipelines: ${pipelines} [{name: "pipeline2", stages: [{class: "com.kmwllc.lucille.stage.CreateChildrenStage"}]}]

dev {
  kafka {
    bootstrapServers: "localhost:9092"
  }

  elastic {
    host: "localhost",
    port: "9200"
    sendEnabled: ${?ELASTIC_SEND_ENABLED}
  }
}

aws {
  accessKeyId: "${?AWS_ACCESS_KEY_ID}",
  secretAccessKey: "${?AWS_SECRET_ACCESS_KEY}",
  defaultRegion: "${?AWS_DEFAULT_REGION}"
}

elastic {
  url: "http://localhost:9200"
  index: "lucille-default"
  type: "lucille-type"
  sendEnabled: false
}

# Solr default mode
solr {
  url: "http://localhost:8983/solr/callbacktest"
}

# Solr cloud mode with url
solr {
  useCloudClient: true
  defaultCollection: "zktest2"
  url: ["http://localhost:8983/solr"]
}

# Solr cloud mode with zkHosts
solr {
  useCloudClient: true
  defaultCollection: "gettingstarted"
  zkHosts: ["zookeeper1:2181", "zookeeper2:2181", "zookeeper3:2181"]

  # optional
  zkChroot: "/solr"
}

opensearch {
  url: "https://admin:admin@localhost:9200"
  url: ${?OPENSEARCH_URL} # allow env override
  index: "lucille-default"
  index: ${?OPENSEARCH_INDEX}
  acceptInvalidCert: false # only enable for testing ssl/https against localhost
}

kafka {
  bootstrapServers: "localhost:9092"

  # how long a kafka poll should wait for data before returning an empty record set
  pollIntervalMs: 250

  # maximum time allowed between kafka polls before consumer is evicted from consumer group
  maxPollIntervalSecs: 600 # 10 minutes

  # ID of consumer group that all lucille workers should belong to
  consumerGroupId: "lucille_workers"

  maxRequestSize: 250000000

}

worker {

  # maximum number of times across all workers that an attempt should be made to process any given document;
  # when this property is not provided, retries will not be tracked and no limit will be imposed;
  # this property requires that zookeeper is available at the specified zookeeper.connectString
  #maxRetries: 2

  # number of worker threads to start for each pipeline when running lucille in local / single-JVM mode
  threads: 2

  # maximum time to spend processing a message, after which the worker will assume a problem and shut down
  # NOTE: not supported while com.kmwllc.lucille.core.Worker and PipelineWorker are being reconciled
  #maxProcessingSecs: 600 # 10 minutes
}

publisher {

  # this setting controls
  #     1) maximum size of the queue of published documents waiting to be processed
  #     2) maximum size of the queue of completed documents waiting to be indexed
  # e.g. if it is set to 10 then each each queue can contain up to 10 documents for a total of 20
  #
  # attempts to publish beyond this capacity will cause the publisher to block
  # this setting applies only when running Lucille in local / single-JVM mode using a LocalMessageManager
  # this setting defaults to 100
  queueCapacity: 100
}

zookeeper {
  connectString: "localhost:2181"
}

log {
  seconds: 30 # how often components (Publisher, Worker, Indexer) should log a status update
}

runner {
  #umncomment to output detailed metrics to the main lucille log at the end of each run
  #metricsLoggingLevel: "INFO"
}