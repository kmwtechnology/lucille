package com.kmwllc.lucille.core;

import static com.kmwllc.lucille.core.Document.RUNID_FIELD;

import com.codahale.metrics.SharedMetricRegistries;
import com.codahale.metrics.Slf4jReporter;
import com.kmwllc.lucille.core.spec.SpecBuilder;
import com.kmwllc.lucille.indexer.IndexerFactory;
import com.kmwllc.lucille.message.*;
import com.kmwllc.lucille.util.LogUtils;
import com.kmwllc.lucille.util.ThreadNameUtils;
import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;
import com.typesafe.config.ConfigRenderOptions;
import com.typesafe.config.ConfigValue;
import java.util.Map.Entry;
import org.apache.commons.cli.*;
import org.apache.commons.lang3.time.StopWatch;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.PrintWriter;
import java.io.StringWriter;
import java.util.*;
import java.util.concurrent.TimeUnit;
import org.slf4j.MDC;

/**
 * Executes a Lucille run. A run is a sequential execution of one or more Connectors.
 * During a run, all the work generated by one Connector must be complete before the next Connector begins.
 * A run should stop if any Connector fails.
 * A Connector is considered to have failed if any of its lifecycle methods throw an Exception.
 * Importantly, a connector is NOT considered to have failed if one or more of the documents it publishes
 * encountere an error during pipeline exeuction or indexing.
 * <p>
 * In a distributed deployment, there will be one or more Worker processes and one or more Indexer processes
 * that stay alive indefinitely and constantly poll for work. A Runner can then execute
 * a sequence of connectors which pump work into the system. The work from this particular "run" should
 * then be handled by the existing Worker and Indexer. The Runner will terminate when all the work
 * it generated was complete, while the Worker and Indexer would stay alive.
 * <p>
 * For simpler deployments, Runner provides a way to execute an end-to-end run
 * without requiring that the Worker and Indexer have been started as separate processes.
 * Instead, the Worker(s) and Indexer are launched as threads in the Runner's own JVM.
 * This can be done in a fully local mode, where all message traffic
 * is stored in in-memory queues, or in Kafka mode, where message traffic flows through an external
 * deployment of Kafka.
 * <p>
 * A local end-to-end run involves a total of 4 threads for each connector:
 * <p>
 * 1) a Worker thread polls for documents to process and runs them through the pipeline
 * 2) an Indexer thread polls for processed documents and indexes them
 * 3) a Connector thread reads data from a source, generates documents, and publishes them;
 * Connectors are run sequentially so there will be at most one Connector thread at any time
 * 4) the main thread launches the other threads, and then uses the Publisher to poll for Events
 * and wait for completion of the run
 */
public class Runner {

  public static final int DEFAULT_CONNECTOR_TIMEOUT = 1000 * 60 * 60 * 24;

  public static final long DEFAULT_WORKER_INDEXER_JOIN_TIMEOUT = 3000;

  private static final Logger log = LoggerFactory.getLogger(Runner.class);

  public enum RunType {
    LOCAL, // launch Worker(s) and Indexer as threads; have all components communicate via in-memory queues
    TEST, // same as LOCAL, but bypass Solr, and store message traffic so it can be inspected after the run
    KAFKA_LOCAL, // launch Worker(s) and Indexer as threads; have all components communicate via Kafka
    KAFKA_DISTRIBUTED // assume Workers/Indexers were started separately (don't launch threads); have all components communicate via Kafka
  }

  // no need to instantiate Runner; all methods currently static
  private Runner() {
  }

  /**
   * Runs the configured connectors. NOTE: parameters are case insensitive.
   * <p>
   * no args: pipelines workers and indexers will be executed in separate threads within the same JVM; communication
   * between components will take place in memory and Kafka will not be used
   * <p>
   * -useKafka: connectors will be run, sending documents and receiving events via Kafka. Pipeline workers
   * and indexers will not be run. The assumption is that these have been deployed as separate processes.
   * <p>
   * -local: modifies -useKafka so that workers and indexers are started as separate threads within the same JVM;
   * kafka is still used for communication between them.
   * <p>
   * -render: prints out the effective/actual config in the exact form it will be seen by Lucille during the run
   */
  public static void main(String[] args) throws Exception {
    Options cliOptions = new Options()
        .addOption(Option.builder("usekafka").hasArg(false)
            .desc("Use Kafka for inter-component communication and don't execute pipelines locally.").build())
        .addOption(Option.builder("local").hasArg(false)
            .desc("Modifies useKafka mode to execute pipelines locally").build())
        .addOption(Option.builder("validate").hasArg(false)
            .desc("Validate the configuration and exit").build())
        .addOption(Option.builder("render").hasArg(false)
            .desc("Print out the configuration file with substitutions applied and exit").build());

    CommandLine cli = null;
    try {
      args = Arrays.stream(args).map(String::toLowerCase).toArray(String[]::new);
      cli = new DefaultParser().parse(cliOptions, args);
    } catch (UnrecognizedOptionException | MissingOptionException e) {
      try (StringWriter writer = new StringWriter();
          PrintWriter printer = new PrintWriter(writer)) {

        String header = "Run a sequence of connectors";
        new HelpFormatter().printHelp(printer, 256, "Runner", header, cliOptions,
            2, 10, "", true);
        log.info(writer.toString());
      }
      System.exit(1);
    }

    Config config = ConfigFactory.load();

    // allow handling of both validate and render flags
    if (cli.hasOption("validate") || cli.hasOption("render")) {
      if (cli.hasOption("render")) {
        renderConfig(config);
      }
      if (cli.hasOption("validate")) {
        runInValidationMode(config);
      }
      return;
    }

    RunType runType = getRunType(cli.hasOption("usekafka"), cli.hasOption("local"));

    // Kick off the run with a log of the result
    RunResult result = runWithResultLog(config, runType);

    if (result.getStatus()) {
      System.exit(0);
    } else {
      System.exit(1);
    }
  }

  /** Utility method to generate a Run ID. */
  public static String generateRunId() {
    return UUID.randomUUID().toString();
  }

  /**
   * Run Lucille end-to-end using the sequence of Connectors and the pipeline defined in the given
   * config file. Stop the run if any of the connectors fails.
   * <p>
   * Run in a local mode where message traffic is kept in memory and there is no
   * communication with external systems like Kafka. Sending to solr is bypassed.
   * <p>
   * Return a TestMessenger for each connector that
   * can be used to review the messages that were sent between various components during that connector's execution.
   */
  public static Map<String, TestMessenger> runInTestMode(String configName) throws Exception {
    Config config = ConfigFactory.load(configName);
    return runInTestMode(config);
  }

  public static Map<String, TestMessenger> runInTestMode(Config config) throws Exception {
    RunResult result = run(config, RunType.TEST);
    return result.getHistory();
  }

  public static Map<String, List<Exception>> runInValidationMode(String configName) throws Exception {
    return runInValidationMode(ConfigFactory.load(configName));
  }

  /**
   * Validates the given Config, including its pipelines, connectors, indexers, and other miscellaneous configuration
   * (Publisher, Runner, Kafka, etc). Returns a map of entry names to lists of Exceptions, representing errors with the
   * configuration. The map uses individual pipeline / connector names for keys, and then "indexer" and "other" for the indexer / other
   * validation issues, respectively. (Keys are only present if there are associated errors.)
   *
   * @param config The configuration you want to validate.
   * @return Returns a map of entry names to lists of Exceptions (errors with the Config).
   * @throws Exception If a larger error occurs, preventing the Config from being validates.
   */
  public static Map<String, List<Exception>> runInValidationMode(Config config) throws Exception {
    // Resolve any potential substitutions once, to prevent errors about missing values.
    config = config.resolve();
    Map<String, List<Exception>> allExceptionsMap = new HashMap<>();

    Map<String, List<Exception>> pipelineExceptions = validatePipelines(config);
    log.info(stringifyValidation(pipelineExceptions, "Pipeline"));

    Map<String, List<Exception>> connectorExceptions = validateConnectors(config);
    log.info(stringifyValidation(connectorExceptions, "Connector"));

    List<Exception> indexerExceptions = validateIndexer(config);
    log.info(stringifyValidation(indexerExceptions, "Indexer"));

    List<Exception> otherParentExceptions = validateOtherParents(config);
    log.info(stringifyValidation(otherParentExceptions, "Other (Publisher, Runner, etc.)"));

    allExceptionsMap.putAll(pipelineExceptions);
    allExceptionsMap.putAll(connectorExceptions);

    // only put "indexer" key in the map if there are errors.
    if (!indexerExceptions.isEmpty()) {
      if (allExceptionsMap.containsKey("indexer")) {
        log.warn("A pipeline or connector was named \"indexer\". Its validation error(s) will be combined with error(s) for indexer configuration.");
        allExceptionsMap.get("indexer").addAll(indexerExceptions);
      } else {
        allExceptionsMap.put("indexer", indexerExceptions);
      }
    }

    // only put "other" key in the map if there are errors.
    if (!otherParentExceptions.isEmpty()) {
      if (allExceptionsMap.containsKey("other")) {
        log.warn("A pipeline or connector was named \"other\". Its validation error(s) will be combined with error(s) for other configuration (publisher, worker, etc.).");
        allExceptionsMap.get("other").addAll(otherParentExceptions);
      } else {
        allExceptionsMap.put("other", otherParentExceptions);
      }
    }

    return allExceptionsMap;
  }

  private static Map<String, List<Exception>> validateConnectors(Config rootConfig) {
    Map<String, List<Exception>> exceptionMap = new LinkedHashMap<>();

    if (!rootConfig.hasPath("connectors")) {
      exceptionMap.put("connectors", List.of(new Exception("Configuration does not contain key \"connectors\".")));
      return exceptionMap;
    }

    Set<String> seenNames = new HashSet<>();
    int idx = 0;

    for (Config connectorConfig : rootConfig.getConfigList("connectors")) {
      idx++;
      String connectorName = connectorConfig.hasPath("name") ? connectorConfig.getString("name") : null;

      List<Exception> exceptionsForConnector = Connector.getConnectorConfigExceptions(connectorConfig);
      if (connectorName != null && !connectorName.isEmpty()) {
        if (!seenNames.add(connectorName)) {
          exceptionMap.computeIfAbsent(connectorName, k -> new ArrayList<>()).add(new Exception("There are multiple connectors with the name " + connectorName));
        }
      }

      if (!exceptionsForConnector.isEmpty()) {
        String key = (connectorName != null && !connectorName.isEmpty()) ? connectorName : "connector[" + idx + "]";
        exceptionMap.computeIfAbsent(key, k -> new ArrayList<>()).addAll(exceptionsForConnector);
      }
    }

    return exceptionMap;
  }

  /**
   * Returns a mapping from pipeline names to the list of exceptions produced when validating them. (only includes
   * a pipeline name if there are errors with its config.)
   */
  private static Map<String, List<Exception>> validatePipelines(Config rootConfig) throws Exception {
    Map<String, List<Exception>> exceptionMap = new LinkedHashMap<>();

    if (!rootConfig.hasPath("pipelines")) {
      exceptionMap.put("pipelines", List.of(new Exception("Configuration does not contain key \"pipelines\".")));
      return exceptionMap;
    }

    for (Config pipelineConfig : rootConfig.getConfigList("pipelines")) {
      String pipelineName = pipelineConfig.getString("name");

      List<Exception> exceptionsForPipeline = Pipeline.validateStages(rootConfig, pipelineName);

      if (!exceptionsForPipeline.isEmpty()) {
        // No checks on whether the map has the pipeline name, since Pipeline checks for duplicate names...
        exceptionMap.put(pipelineName, exceptionsForPipeline);
      }
    }

    return exceptionMap;
  }

  private static List<Exception> validateIndexer(Config config) {
    try {
      IndexerFactory.fromConfig(config, new LocalMessenger(), true, "");
    } catch (Exception e) {
      return List.of(e);
    }

    return List.of();
  }

  private static List<Exception> validateOtherParents(Config rootConfig) {
    List<Exception> exceptions = new ArrayList<>();

    Config configPropertyConfig = ConfigFactory.parseResourcesAnySyntax("validConfigProperties.conf");

    // calling entrySet on the root() means it returns the keys - publisher, log, etc... and not entries
    // for paths to each of the individual values.
    for (Entry<String, ConfigValue> entry : configPropertyConfig.root().entrySet()) {
      String optionalParentName = entry.getKey();

      // from this config, we can get required / optional properties
      if (!rootConfig.hasPath(optionalParentName)) {
        continue;
      }

      Config parentPropertyConfig = configPropertyConfig.getConfig(optionalParentName);

      SpecBuilder specBuilder = SpecBuilder.withoutDefaults();

      if (parentPropertyConfig.hasPath("optionalProperties")) {
        List<String> optionalProperties = parentPropertyConfig.getStringList("optionalProperties");
        specBuilder.withOptionalProperties(optionalProperties.toArray(new String[0]));
      }

      if (parentPropertyConfig.hasPath("requiredProperties")) {
        List<String> requiredProperties = parentPropertyConfig.getStringList("requiredProperties");
        specBuilder.withRequiredProperties(requiredProperties.toArray(new String[0]));
      }

      try {
        specBuilder.build().validate(rootConfig.getConfig(optionalParentName), optionalParentName);
      } catch (IllegalArgumentException e) {
        exceptions.add(e);
      }
    }

    return exceptions;
  }

  public static void renderConfig(Config config) {
    ConfigRenderOptions renderOptions = ConfigRenderOptions.defaults()
        .setJson(true)
        .setComments(false)
        .setOriginComments(false);
    log.info(config.root().render(renderOptions));
  }

  /**
   * Derives the RunType for the new run from the 'useKafka' and 'local' parameters.
   */
  static RunType getRunType(boolean useKafka, boolean local) {
    if (useKafka) {
      if (local) {
        return RunType.KAFKA_LOCAL;
      } else {
        return RunType.KAFKA_DISTRIBUTED;
      }
    } else {
      return RunType.LOCAL;
    }
  }

  public static RunResult runWithResultLog(Config config, RunType runType) throws Exception {
    return runWithResultLog(config, runType, null);
  }

  /**
   * Kicks off a new Lucille run and logs information about the run to the console after completion.
   */
  public static RunResult runWithResultLog(Config config, RunType runType, String runId)
      throws Exception {
    StopWatch stopWatch = new StopWatch();
    stopWatch.start();
    RunResult result;

    try {
      result = run(config, runType, runId);

      // log detailed metrics
      Slf4jReporter.forRegistry(SharedMetricRegistries.getOrCreate(LogUtils.METRICS_REG))
          .outputTo(log).withLoggingLevel(getMetricsLoggingLevel(config)).build().report();
      // log run summary
      log.info(result.toString());
      return result;
    } finally {
      stopWatch.stop();
      log.info(String.format("Run took %.2f secs.",
          (double) stopWatch.getTime(TimeUnit.MILLISECONDS) / 1000));
    }
  }

  /**
   * Non managed Run with internal generated runId
   *
   * @param config
   * @param type
   * @return
   * @throws Exception
   */
  public static RunResult run(Config config, RunType type) throws Exception {
    return run(config, type, null);
  }

  /**
   * Generates a run ID if not supplied and performs an end-to-end run of the designated type.
   *
   * @param config
   * @param type
   * @param runId
   * @return
   * @throws Exception
   */
  public static RunResult run(Config config, RunType type, String runId) throws Exception {
    if (runId == null) {
      runId = Runner.generateRunId();
    }

    MDC.put(RUNID_FIELD, runId);

    Map<String, List<Exception>> validationErrors = runInValidationMode(config);
    if (!validationErrors.isEmpty()) {
      log.error("Pre-run validation failed.");

      Map<String, TestMessenger> history =
          type.equals(RunType.TEST) ? new HashMap<>() : null;

      return new RunResult(false,
          Collections.emptyList(), Collections.emptyList(), history, runId);
    }

    log.info("Starting run with id " + runId);

    List<Connector> connectors = Connector.fromConfig(config);
    List<ConnectorResult> connectorResults = new ArrayList<>();

    boolean startWorkerAndIndexer = !type.equals(RunType.KAFKA_DISTRIBUTED);
    boolean bypassSolr = type.equals(RunType.TEST);

    Map<String, TestMessenger> history = type.equals(RunType.TEST) ? new HashMap<>() : null;

    for (Connector connector : connectors) {

      WorkerMessengerFactory workerMessengerFactory;
      IndexerMessengerFactory indexerMessengerFactory;
      PublisherMessengerFactory publisherMessengerFactory;

      if (RunType.TEST.equals(type)) {
        TestMessenger messenger = new TestMessenger();
        history.put(connector.getName(), messenger);
        workerMessengerFactory = WorkerMessengerFactory.getConstantFactory(messenger);
        indexerMessengerFactory = IndexerMessengerFactory.getConstantFactory(messenger);
        publisherMessengerFactory = PublisherMessengerFactory.getConstantFactory(messenger);
      } else if (RunType.LOCAL.equals(type)) {
        LocalMessenger messenger = new LocalMessenger(config);
        workerMessengerFactory = WorkerMessengerFactory.getConstantFactory(messenger);
        indexerMessengerFactory = IndexerMessengerFactory.getConstantFactory(messenger);
        publisherMessengerFactory = PublisherMessengerFactory.getConstantFactory(messenger);
      } else { // RunType.KAFKA_LOCAL.equals(type) || RunType.KAFKA_DISTRIBUTED.equals(type)
        workerMessengerFactory = WorkerMessengerFactory.getKafkaFactory(config, connector.getPipelineName());
        indexerMessengerFactory = IndexerMessengerFactory.getKafkaFactory(config, connector.getPipelineName());
        publisherMessengerFactory = PublisherMessengerFactory.getKafkaFactory(config);
      }

      ConnectorResult result =
          runConnectorWithComponents(config, runId, type, connector,
              workerMessengerFactory, indexerMessengerFactory, publisherMessengerFactory, startWorkerAndIndexer, bypassSolr);

      connectorResults.add(result);

      if (!result.getStatus()) {
        log.error("Aborting run because " + connector.getName() + " failed.");
        return new RunResult(false, connectors, connectorResults, history, runId);
      }
    }

    return new RunResult(true, connectors, connectorResults, history, runId);
  }

  /**
   * Runs the designated Connector. Returns a successful ConnectorResult only when 1) the Connector has
   * finished generating documents, and 2) all of the documents (and any generated children) have reached
   * an end state in the workflow: either being indexed or erroring-out. Returns a failing ConnectorResult
   * if any connector lifecycle method throws an exception or the publisher itself fails
   */
  public static ConnectorResult runConnector(Config config, String runId, Connector connector, Publisher publisher) {
    try {
      return runConnectorInternal(config, runId, connector, publisher);
    } finally {
      try {
        connector.close();
      } catch (Exception e) {
        log.error("Error closing connector", e);

        if (publisher != null) {
          try {
            publisher.close();
          } catch (Exception e2) {
            log.error("Error closing publisher", e2);
            return new ConnectorResult(connector, publisher, false, "Error closing connector and publisher");
          }
        }

        return new ConnectorResult(connector, publisher, false, "Error closing connector");
      }

      if (publisher != null) {
        try {
          publisher.close();
        } catch (Exception e) {
          log.error("Error closing publisher", e);
          return new ConnectorResult(connector, publisher, false, "Error closing publisher");
        }
      }
    }
  }

  /**
   * Helper used by runConnector(). Performs the following steps:
   * 1) call connector.preExecute()
   * 2) launch a thread that calls connector.execute() with the given publisher, and then publisher.flush()
   * 3) call publisher.waitForCompletion()
   * 4) call connector.postExecute()
   */
  private static ConnectorResult runConnectorInternal(Config config, String runId, Connector connector, Publisher publisher) {
    log.info("Running connector " + connector.getName() + " feeding to pipeline " +
        (connector.getPipelineName() == null ? "NOT CONFIGURED" : connector.getPipelineName()));
    StopWatch stopWatch = new StopWatch();
    stopWatch.start();

    try {
      connector.preExecute(runId);
    } catch (ConnectorException e) {
      log.error("Connector failed to perform pre execution actions.", e);
      return new ConnectorResult(connector, publisher, false, "preExecute failed.");
    }

    PublisherResult pubResult = null;

    // the publisher could be null if we are running a connector that has no associated pipeline and therefore
    // there's nothing to publish to; in this case, we call execute synchronously because we are not
    // pumping any work into the system and waiting for other components to complete it
    if (publisher == null) {
      try {
        connector.execute(null);
      } catch (ConnectorException e) {
        log.error("Connector execution failed.", e);
        return new ConnectorResult(connector, publisher, false, "execute failed.");
      }
    } else {
      try {
        ConnectorThread connectorThread = new ConnectorThread(connector, publisher, runId, ThreadNameUtils.createName("Connector", runId));
        connectorThread.start();
        final int connectorTimeout = config.hasPath("runner.connectorTimeout") ?
            config.getInt("runner.connectorTimeout") : DEFAULT_CONNECTOR_TIMEOUT;
        pubResult = publisher.waitForCompletion(connectorThread, connectorTimeout);
      } catch (Exception e) {
        log.error("waitForCompletion failed", e);
        return new ConnectorResult(connector, publisher, false, "waitForCompletion failed");
      }
    }

    if (pubResult == null || pubResult.getStatus()) {
      try {
        connector.postExecute(runId);
      } catch (ConnectorException e) {
        log.error("postExecute failed", e);
        return new ConnectorResult(connector, publisher, false, "postExecute failed");
      }
    }

    stopWatch.stop();
    double durationSecs = ((double) stopWatch.getTime(TimeUnit.MILLISECONDS)) / 1000;
    log.info(String.format("Connector %s feeding to pipeline %s complete. Time: %.2f secs.",
        connector.getName(), connector.getPipelineName(), durationSecs));

    boolean status = pubResult == null ? true : pubResult.getStatus();
    String msg = pubResult == null ? null : pubResult.getMessage();
    return new ConnectorResult(connector, publisher, status, msg, durationSecs);
  }

  /**
   * Wrapper around runConnector() that starts and stops the other components necessary for a complete connector
   * execution; specifically, a WorkerPool and an Indexer.
   */
  private static ConnectorResult runConnectorWithComponents(Config config,
      String runId,
      RunType runType,
      Connector connector,
      WorkerMessengerFactory workerMessengerFactory,
      IndexerMessengerFactory indexerMessengerFactory,
      PublisherMessengerFactory publisherMessengerFactory,
      boolean startWorkerAndIndexer,
      boolean bypassIndexer) throws Exception {
    String pipelineName = connector.getPipelineName();
    WorkerPool workerPool = null;
    Indexer indexer = null;
    Thread indexerThread = null;
    Publisher publisher = null;

    try {
      // If local/test we want to give WorkerPool/Indexer the run_id directly.
      // (Otherwise let Kafka messengers pass it thru in the documents.)
      String localRunId = (runType.equals(RunType.LOCAL) || runType.equals(RunType.TEST)) ? runId : null;

      // create a common metrics naming prefix to be used by all components that will be collecting metrics,
      // to ensure that metrics are collected separately for each connector/pipeline pair
      String metricsPrefix = runId + "." + connector.getName() + "." + connector.getPipelineName();

      if (startWorkerAndIndexer && connector.getPipelineName() != null) {
        workerPool = new WorkerPool(config, pipelineName, localRunId, workerMessengerFactory, metricsPrefix);

        try {
          workerPool.start();
        } catch (Exception e) {
          log.error("Error starting workers for pipeline " + pipelineName, e);
          return new ConnectorResult(connector, publisher, false, "Error starting workers for pipeline " + pipelineName);
        }

        try {
          IndexerMessenger indexerMessenger = indexerMessengerFactory.create();
          indexer = IndexerFactory.fromConfig(config, indexerMessenger, bypassIndexer, metricsPrefix, localRunId);
        } catch (Exception e) {
          log.error("Error creating indexer from config.", e);
          return new ConnectorResult(connector, publisher, false, "Error creating indexer from config.");
        }

        if (!indexer.validateConnection()) {
          String msg = "Indexer could not connect.";
          log.error(msg);
          // clean up indexer lifecycle which was created but never run in a thread.
          indexer.closeConnection();
          return new ConnectorResult(connector, publisher, false, msg);
        }

        indexerThread = new Thread(indexer, ThreadNameUtils.createName("Indexer", localRunId));
        indexerThread.start();
      }

      if (connector.getPipelineName() != null) {
        PublisherMessenger publisherMessenger = publisherMessengerFactory.create();
        publisher = new PublisherImpl(config, publisherMessenger, runId,
            connector.getPipelineName(), metricsPrefix, connector.requiresCollapsingPublisher());
      }

      return runConnector(config, runId, connector, publisher);

    } finally {
      if (workerPool != null) {
        workerPool.stop();
        workerPool.join(DEFAULT_WORKER_INDEXER_JOIN_TIMEOUT);
      }
      if (indexerThread != null) {
        indexer.terminate();
        indexerThread.join(DEFAULT_WORKER_INDEXER_JOIN_TIMEOUT);
      }
    }
  }

  /**
   * Returns a stringified version of the given map of exceptions, which is mapping individual "elements" (usually, pipelines) to their associated
   * exceptions. Map may be empty. Uses the given validation name in logged messages to clarify which element of the Lucille Config is being validated.
   */
  // Package access for unit test
  static String stringifyValidation(Map<String, List<Exception>> exceptions, String validationName) {
    if (exceptions.isEmpty()) {
      return validationName + " Configuration is valid.";
    } else {
      StringBuilder message = new StringBuilder(validationName + " Configuration is invalid. See exceptions for each element:\n");

      for (Map.Entry<String, List<Exception>> entry : exceptions.entrySet()) {
        message.append("\t").append(entry.getKey()).append(":").append("\n");

        for (Exception e : entry.getValue()) {
          message.append("\t\t").append(e.getMessage()).append("\n");
        }
      }
      return message.delete(message.length() - 1, message.length()).toString();
    }
  }

  /**
   * Returns a stringified version of the given list of exceptions. List may be empty. Uses the given validation name in logged messages
   * to clarify which element of the Lucille Config is being validated.
   */
  // package access for unit test
  static String stringifyValidation(List<Exception> exceptions, String validationName) {
    if (exceptions.isEmpty()) {
      return validationName + " Configuration is valid.";
    } else {
      StringBuilder message = new StringBuilder(validationName + " Configuration is invalid. Errors:\n");

      for (Exception exc : exceptions) {
        message.append("\t").append(exc.getMessage()).append("\n");
      }
      return message.delete(message.length() - 1, message.length()).toString();
    }
  }

  private static Slf4jReporter.LoggingLevel getMetricsLoggingLevel(Config config) {
    try {
      return config.hasPath("runner.metricsLoggingLevel") ?
          Slf4jReporter.LoggingLevel.valueOf(config.getString("runner.metricsLoggingLevel")) :
          Slf4jReporter.LoggingLevel.DEBUG;
    } catch (Exception e) {
      log.error("Error obtaining metrics logging level", e);
    }
    return Slf4jReporter.LoggingLevel.DEBUG;
  }
}
